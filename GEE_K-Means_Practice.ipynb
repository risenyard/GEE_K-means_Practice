{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Exercise #2\n",
    "\n",
    "In this graded group exercise, you will use the **GEE Python API** and the **geemap Python package** to:\n",
    "\n",
    "- Retrieve a collection of satellite images over a study area\n",
    "- Create a composite image and visualize it\n",
    "- Apply a simple unsupervised machine learning algorithm (e.g., k-means) for clustering the study area\n",
    "- Analyze the clustering results given a set of reference points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Points of Discussions\n",
    "\n",
    "- Why do we need to train an unsupervised classifier in GEE?\n",
    "- Impact of the number clusters on the results\n",
    "- Impact of performing the clustering in region X and applying it on region Y\n",
    "  - Region Y has the same land cover classes present in Region X\n",
    "  - Region Y has more land cover classes/types.\n",
    "- What can you say about the computational time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Task Implementation (for Turkey)\n",
    "\n",
    "### 1.1 Define a case study area \n",
    "*The size of bounding box covering the study area should be large enough (e.g., few hundred kilometers).* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start the process, the initialization of the API needs to be done.\n",
    "- Two packages are needed: `ee` and `geemap`\n",
    "- The authentication line is necessary to access the resources in Google Earth Engine. And it should be run at first and follow the instructions. Then the Python environment knows the authentication and it needn't be executed anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "import geemap\n",
    "import sys\n",
    "\n",
    "#   Please run this just once to get access to the resources in GEE\n",
    "#ee.Authenticate()\n",
    "\n",
    "try:\n",
    "    # Initialize the library\n",
    "    ee.Initialize()\n",
    "    print('Google Earth Engine has initialized successfully!')\n",
    "except ee.EEException as e:\n",
    "    print('Google Earth Engine has failed to initialize!')\n",
    "except:\n",
    "    print(\"Unexpected error:\", sys.exc_info()[0])\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select Turkey as the area of interest and create its bounding box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bounding box of Turkey (Define the region of interest as a polygon)\n",
    "left = 28.0391 #25.663\n",
    "right = 32.1919#44.822\n",
    "up= 37.5306 #42.366\n",
    "down = 39.4904 #35.817 \n",
    "roi = ee.Geometry.Polygon(\n",
    "        [[[left, down],\n",
    "          [right, down],\n",
    "          [right, up],\n",
    "          [left, up],]], None, False)\n",
    "\n",
    "bbox = roi.bounds().getInfo()['coordinates'][0]\n",
    "print(bbox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we center Turkey at the center of the map and add the bounding box as a layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a map centered on Turkey\n",
    "map = geemap.Map(center=[(up+down)/2, (left+right)/2], zoom=6)\n",
    "\n",
    "# Add the bounding box to the map\n",
    "bbox_layer = geemap.geojson_to_ee({\n",
    "    'type': 'Feature',\n",
    "    'geometry': {\n",
    "        'type': 'Polygon',\n",
    "        'coordinates': [bbox]\n",
    "    },\n",
    "    'properties': {\n",
    "        'name': 'Bounding box'\n",
    "    }\n",
    "})\n",
    "map.addLayer(bbox_layer, {'color': 'grey'}, 'Bounding box')\n",
    "\n",
    "# Display the map\n",
    "map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Add the reference points\n",
    "*Filter the reference points to the selected region of interest*\n",
    "\n",
    "- The reference points are stored in the multiple .csv file under /Reference_Point\n",
    "- We read them as DataFrame first and then convert it to ee FeatureCollection.\n",
    "\n",
    "NB! Task was solved with three solutions (A, B, C), each of which runs parallelly with the others. So you just need to run one of them for the whole assignment. For this document, we ran Solution C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Solution A\n",
    "- As Google Earth API cannot handle external data larger than 10Mb, so we need to avoid making a big complete ee FeatureColleciton of all points and handle it by ee package. Instead, we need to decrease the size of the ee FeatureCollection.\n",
    "- Therefore, here we read all the points into dataframe first and finishe the filtering when it was still a dataframe.\n",
    "- Then, we convert this smaller dataframe into ee FeatureCollection.\n",
    "- This method took 9.1 seconds to finish on my laptop locally. (We need Internet connection after the conversion from dataframe, like half the process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dask.dataframe as dd\n",
    "# import pandas as pd\n",
    "\n",
    "# # Use DASK to read all csv together\n",
    "# da = dd.read_csv('Reference_Point/SamplesSet*.csv')\n",
    "# ref_point = da.compute()\n",
    "\n",
    "# # Get rid of the rows with no landcover\n",
    "# ref_point = ref_point.dropna(subset=['landcover'])\n",
    "\n",
    "# # Filter the data withi the area of interest\n",
    "# ref_point_sub = ref_point[(ref_point['lon'] >= left) & (ref_point['lon'] <= right) & (ref_point['lat'] >= down) & (ref_point['lat'] <= up)]\n",
    "\n",
    "# print(ref_point_sub)\n",
    "\n",
    "# # Convert the data to a ee feature\n",
    "# def row_to_feature(row):\n",
    "#     point = ee.Geometry.Point(row['lon'], row['lat'])\n",
    "#     feature = ee.Feature(point, {'landcover': row['landcover']})\n",
    "#     return feature\n",
    "\n",
    "# # Make feature collection\n",
    "# feature_list = ref_point_sub.apply(row_to_feature,axis=1).tolist()\n",
    "# feature_collection = ee.FeatureCollection(feature_list)\n",
    "# ref_point_ee = feature_collection.filter(ee.Filter.geometry(roi))\n",
    "# print(ref_point_ee.size().getInfo())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 Solution B\n",
    "NB. the solution also exceeds the 10Mb limit, so it is not the best solution.\n",
    "\n",
    "- Same reason for Solution A, Google Earth API cannot handle external data larger than 10Mb. Solution B tries to avoid online calculation with big dataset by making loops to handle the process separately. \n",
    "- Here the way could be loop around each csv file, convert it into ee FeatureCollection, filter it and then merge the selected point with the result of last loop.\n",
    "- This solution keeps calling Google Earth, therefore robust net connection for the whole process is necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dask.dataframe as dd\n",
    "# import pandas as pd\n",
    "\n",
    "# # Convert the data to a ee feature\n",
    "# def row_to_feature(row):\n",
    "#     point = ee.Geometry.Point(row['lon'], row['lat'])\n",
    "#     feature = ee.Feature(point, {'landcover': row['landcover']})\n",
    "#     return feature\n",
    "\n",
    "# ref_point= pd.DataFrame({})\n",
    "# ref_point_ee= ee.FeatureCollection([])\n",
    "# # loop each csv and run the process separately\n",
    "# for i in range(1,11):\n",
    "#     path= 'Reference_Point/SamplesSet'+ str(i) +'.csv'\n",
    "#     ref_point_sub = pd.read_csv(path)\n",
    "#     ref_point_sub = ref_point_sub.dropna(subset=['landcover'])\n",
    "#     # merge the dataframe\n",
    "#     ref_point = pd.concat([ref_point,ref_point_sub])\n",
    "#     # convert the dataframe to ee feature\n",
    "#     feature_list = ref_point_sub.apply(row_to_feature,axis=1).tolist()\n",
    "#     feature_collection = ee.FeatureCollection(feature_list)\n",
    "#     filtered_collection = feature_collection.filter(ee.Filter.geometry(roi))\n",
    "#     # merge the feature collection\n",
    "#     ref_point_ee = ref_point_ee.merge(filtered_collection)\n",
    "#     # print(ref_point_ee.size().getInfo())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3 Solution C\n",
    "- Solution C avoids online calculating with big dataset by using geopandas to handle geo objects.\n",
    "- We could add the points as geodataframe and filter it with the area of interest locally with geopandas\n",
    "- And then we transfer the geodataframe into ee. objest. So the input for the geemap occupies a much smaller storage.\n",
    "- This solution takes 6.9 seconds. It just needs Internet for the last step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# use DASK to read all csv together\n",
    "da = dd.read_csv('ref_pts/ref_pts/SamplesSet*.csv')\n",
    "ref_point = da.compute()\n",
    "\n",
    "# Get rid of the rows with no landcover\n",
    "ref_point = ref_point.dropna(subset=['landcover'])\n",
    "\n",
    "# Convert the dataframe to geodataframe\n",
    "geometry = [Point(xy) for xy in zip(ref_point['lon'], ref_point['lat'])]\n",
    "gdf = gpd.GeoDataFrame(ref_point, geometry=geometry, crs='EPSG:4326')\n",
    "gdf = gdf.drop(['lon', 'lat'], axis=1)\n",
    "\n",
    "# Filter the points in the area of interest\n",
    "area_of_interest = Polygon([(left, down), (right, down), (right, up), (left, up)])\n",
    "gdf_ref_point_sub = gdf.cx[left:right, down:up]\n",
    "\n",
    "# Convert the geodataframe to ee\n",
    "ref_point_ee = geemap.geopandas_to_ee(gdf_ref_point_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 & 1.4. Generate image collection & Calculate the median image\n",
    "Generate Sentinel 2 images for this region of interest covering the summer of 2023\n",
    "Reduce this image collection by calculating the median of all values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To mask clouds in Sentinel-2 image using QA band\n",
    "def mask_s2_clouds(image):\n",
    "  qa = image.select('QA60')\n",
    "  # Bits 10 and 11 are clouds and cirrus, respectively.\n",
    "  cloud_bit_mask = 1 << 10\n",
    "  cirrus_bit_mask = 1 << 11\n",
    "  # Both flags should be set to zero, indicating clear conditions.\n",
    "  mask = (\n",
    "      qa.bitwiseAnd(cloud_bit_mask)\n",
    "      .eq(0)\n",
    "      .And(qa.bitwiseAnd(cirrus_bit_mask).eq(0))\n",
    "  )\n",
    "  # Return the image after the mask is applied and divide it by 10000 to scale the values around 0-1 \n",
    "  return image.updateMask(mask).divide(10000)\n",
    "\n",
    "# Get the Sentinel-2 image collection for the region of interest\n",
    "image = (\n",
    "     ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED') \n",
    "    .filterBounds(roi) \n",
    "    # Summer 2023\n",
    "    .filterDate('2023-06-01', '2023-08-31') \n",
    "    # Pre-filter to get less cloudy granules.\n",
    "    .filterMetadata('CLOUDY_PIXEL_PERCENTAGE', 'less_than', 10) \n",
    "    .map(mask_s2_clouds)\n",
    "    # Get the median of the image collection\n",
    "    .median()\n",
    ")\n",
    "\n",
    "# Set visualization parameters for visulization\n",
    "vis_params = {\n",
    "    'min': 0,'max': 0.4,\n",
    "    'bands': [\"B4\", \"B3\", \"B2\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Visualize the reference points and image\n",
    "*Make the image and visualisation together on one map*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the 'Bounding box' layer and hide it\n",
    "bbox_layer = map.find_layer('Bounding box')\n",
    "bbox_layer.visible = False\n",
    "\n",
    "# Set visualization parameters for visulization of image\n",
    "vis_params = {\n",
    "    'min': 0,'max': 0.4,\n",
    "    'bands': [\"B4\", \"B3\", \"B2\"]\n",
    "}\n",
    "# Add the image to the map\n",
    "map.addLayer(image, vis_params, 'S2-2023-Median')\n",
    "\n",
    "# Add the reference points to the map\n",
    "map.addLayer(ref_point_ee, {}, 'Area_of_Interest')\n",
    "map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6. Make cluster\n",
    "- *Cluster the selected image into a predefined number of clusters*\n",
    "- *You can use the reference points labels to get a first estimation of number of clusters and k-mean as the clustering algorithm*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the sample points to train the K-means model\n",
    "training = image.sample(**{\n",
    "    'scale': 30,\n",
    "    # 8000 points in total\n",
    "    'numPixels': 8000,\n",
    "    'seed': 0,\n",
    "    'geometries': True, \n",
    "    'region': roi,\n",
    "    # set the tilescan to reduce the occupancy of the storage\n",
    "    'tileScale': 4\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the cluster number as 5 (the reference points have 5 types of landcover)\n",
    "cluster_number = 5\n",
    "clusterer = ee.Clusterer.wekaKMeans(cluster_number).train(training)\n",
    "\n",
    "# Cluster the input using the trained clusterer.\n",
    "image_clustered = image.cluster(clusterer)\n",
    "\n",
    "# Display the clusters with random colors.\n",
    "map.addLayer(image_clustered.randomVisualizer(), {}, 'clusters')\n",
    "map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7. Analyze the clustering results. \n",
    "For example, you can use histogram of the land cover classes per cluster id generated by the k-means algorithm.\n",
    "\n",
    "- First, we join the cluster id into the refernce points based on the geometries and then do visualization for clustering results\n",
    "- Per Cluster ID, we visualised the distribution of different land cover groups (x-axis is different cluster, y-axis is number of reference points of this land cover on the cluster)\n",
    "- We also calculted the V-measure index (a comprehensive index evaluating the homogeneity in one cluster and the completeness from different clusters) to quantify the quality of the clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Join the value of clustered image (cluster_id) into reference point\n",
    "joined_ref_point_ee = image_clustered.reduceRegions(\n",
    "    collection=ref_point_ee,\n",
    "    reducer=ee.Reducer.mean(),\n",
    "    crs='EPSG:4326',\n",
    "    scale = 30,\n",
    "    tileScale = 4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the cluster id into the refernce points based on the geometries and then do visualization for clustering results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the reference point with joined cluster_id into geodataframe for visualisation\n",
    "joined_ref_point_geop = geemap.ee_to_geopandas(joined_ref_point_ee)\n",
    "joined_ref_point_geop = joined_ref_point_geop.rename(columns={\"mean\": \"cluster_id\"})\n",
    "joined_ref_point_geop.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(1,1)\n",
    "# Define a color map for each cluster_id\n",
    "color_map = {\n",
    "    0: 'red',\n",
    "    1: 'blue',\n",
    "    2: 'green',\n",
    "    3: 'yellow',\n",
    "    4: 'purple'\n",
    "}\n",
    "# Split and plot data for each cluster_id\n",
    "for cluster, color in color_map.items():\n",
    "    subset = joined_ref_point_geop[joined_ref_point_geop['cluster_id'] == cluster]\n",
    "    subset.plot(ax=ax, color=color, label=f\"Cluster {cluster}\")\n",
    "# Add legend\n",
    "ax.legend(title=\"Cluster ID\", loc='upper left', bbox_to_anchor=(1, 1))\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.title(label=\"Assigned Cluster of Refernence Points with 5 Clusters\", fontsize=12, color=\"Black\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "joined_ref_point_geop.plot(ax = ax, column= 'landcover', legend = True,  legend_kwds={'title': 'Landcover Type' ,'bbox_to_anchor':(1.1, 0.8),} )\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.title(label=\"Assigned Cluster of Refernence Points with 5 Clusters\", fontsize=12, color=\"Black\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert the reference point with joined cluster_id into dataframe\n",
    "joined_ref_point= joined_ref_point_geop.drop(['geometry'], axis=1)\n",
    "joined_ref_point.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise and evaluate the clustering result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# reshape the dataframe to pivot-table (to calculate the number of different land covers per cluster id)\n",
    "pivot_table = joined_ref_point.pivot_table(index=\"landcover\", columns=\"cluster_id\", values=None, aggfunc=\"size\")\n",
    "\n",
    "# build up the sub plots\n",
    "fig, axs = plt.subplots(ncols=1, nrows=len(pivot_table.columns), figsize=(9, 25))\n",
    "\n",
    "# to visualise the charts\n",
    "for i, column in enumerate(pivot_table.columns):\n",
    "    axs[i].bar(pivot_table.index, pivot_table[column])\n",
    "    axs[i].set_xlabel(\"cluster_id: \"+str(column))\n",
    "    axs[i].set_ylabel(\"landcover count\")\n",
    "    #axs[i].set_title(column)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the v-measure score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import v_measure_score\n",
    "\n",
    "#to calculate the v-measure for the clustering (range of v-measure is 0-1, 1 stands for better interpretation of real situation )\n",
    "v_measure = v_measure_score(joined_ref_point['landcover'], joined_ref_point['cluster_id'])\n",
    "print(\"V-measure: \", v_measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.8. Change the number of clusters and repeat the analysis\n",
    "**This part is documented in the *2.Task Implementation (Rerun for Germany)* section and *3.Discussion* section below.**\n",
    "- For Point of Dicussion 2, we run the code with different number of clusters, get the V-measure index and compare (also compare time)\n",
    "- For Point of Dicussion 3, we run the training in a smaller/similar area similar with less/same land cover types first. Then apply it to our Turkey and compare the V-measure index. (also compare time)\n",
    "- For Point of Dicussion 4, we first record time in Solution A,B,C at chapter 2. Then we also record time for different number of clusters and time resulting from applying model to Turkey with different training models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Task Implementation (Rerun for Germany)\n",
    "The following code was re run to apply clusterer trained with Turkey to Germany. As there are details description in *1 Task Implementation* section, we will not repeat it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try it on Germany\n",
    "# Bounding box of Germany (Define the region of interest as a polygon)\n",
    "left = 7.7875 #25.663 7.7875\n",
    "right = 12.2260#44.822 12.2260\n",
    "up= 50.5574 #42.366  50.5574\n",
    "down = 53.2014 # 53.201435.817 \n",
    "roi_2 = ee.Geometry.Polygon(\n",
    "        [[[left, down],\n",
    "          [right, down],\n",
    "          [right, up],\n",
    "          [left, up],]], None, False)\n",
    "\n",
    "bbox_2 = roi_2.bounds().getInfo()['coordinates'][0]\n",
    "print(bbox_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a map centered on Germany\n",
    "map2 = geemap.Map(center=[(up+down)/2, (left+right)/2], zoom=6)\n",
    "\n",
    "# Add the bounding box to the map\n",
    "bbox_layer_2 = geemap.geojson_to_ee({\n",
    "    'type': 'Feature',\n",
    "    'geometry': {\n",
    "        'type': 'Polygon',\n",
    "        'coordinates': [bbox_2]\n",
    "    },\n",
    "    'properties': {\n",
    "        'name': 'Bounding box'\n",
    "    }\n",
    "})\n",
    "map2.addLayer(bbox_layer_2, {'color': 'grey'}, 'Bounding box')\n",
    "\n",
    "# Display the map\n",
    "map2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To mask clouds in Sentinel-2 image using QA band\n",
    "def mask_s2_clouds(image):\n",
    "  qa = image.select('QA60')\n",
    "  # Bits 10 and 11 are clouds and cirrus, respectively.\n",
    "  cloud_bit_mask = 1 << 10\n",
    "  cirrus_bit_mask = 1 << 11\n",
    "  # Both flags should be set to zero, indicating clear conditions.\n",
    "  mask = (\n",
    "      qa.bitwiseAnd(cloud_bit_mask)\n",
    "      .eq(0)\n",
    "      .And(qa.bitwiseAnd(cirrus_bit_mask).eq(0))\n",
    "  )\n",
    "  # Return the image after the mask is applied and divide it by 10000 to scale the values around 0-1 \n",
    "  return image.updateMask(mask).divide(10000)\n",
    "\n",
    "# Get the Sentinel-2 image collection for the region of interest\n",
    "image_2 = (\n",
    "     ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED') \n",
    "    .filterBounds(roi_2) \n",
    "    # Summer 2023\n",
    "    .filterDate('2023-06-01', '2023-08-31') \n",
    "    # Pre-filter to get less cloudy granules.\n",
    "    .filterMetadata('CLOUDY_PIXEL_PERCENTAGE', 'less_than', 20) #very cloudy in Germany, had to increase tolerance \n",
    "    .map(mask_s2_clouds)\n",
    "    # Get the median of the image collection\n",
    "    .median()\n",
    ")\n",
    "# Set visualization parameters for visulization\n",
    "vis_params = {\n",
    "    'min': 0,'max': 0.4,\n",
    "    'bands': [\"B4\", \"B3\", \"B2\"]\n",
    "}\n",
    "\n",
    "map2.addLayer(image_2, vis_params, 'S2-2019-Median')\n",
    "map2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cluster the input using the trained clusterer.\n",
    "image_clustered_2 = image_2.cluster(clusterer)\n",
    "\n",
    "# Display the clusters with random colors.\n",
    "map2.addLayer(image_clustered_2.randomVisualizer(), {}, 'clusters')\n",
    "map2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#german_ref_points\n",
    "\n",
    "# Filter the points in the area of interest\n",
    "area_of_interest = Polygon([(left, down), (right, down), (right, up), (left, up)])\n",
    "gdf_ref_point_sub = gdf.cx[left:right, down:up]\n",
    "\n",
    "# Convert the geodataframe to ee\n",
    "ref_point_ee = geemap.geopandas_to_ee(gdf_ref_point_sub)\n",
    "\n",
    "# Add the points to the map\n",
    "map2.addLayer(ref_point_ee, {}, 'Area_of_Interest')\n",
    "map2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Join the value of clustered image (cluster_id) into reference point\n",
    "joined_ref_point_ee = image_clustered_2.reduceRegions(\n",
    "    collection=ref_point_ee,\n",
    "    reducer=ee.Reducer.mean(),\n",
    "    crs='EPSG:4326',\n",
    "    scale = 30,\n",
    "    tileScale = 4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_ref_point_geop = geemap.ee_to_geopandas(joined_ref_point_ee)\n",
    "joined_ref_point_geop = joined_ref_point_geop.rename(columns={\"mean\": \"cluster_id\"})\n",
    "joined_ref_point_geop.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(1,1)\n",
    "# Define a color map for each cluster_id\n",
    "color_map = {\n",
    "    0: 'red',\n",
    "    1: 'blue',\n",
    "    2: 'green',\n",
    "    3: 'yellow',\n",
    "    4: 'purple'\n",
    "}\n",
    "# Split and plot data for each cluster_id\n",
    "for cluster, color in color_map.items():\n",
    "    subset = joined_ref_point_geop[joined_ref_point_geop['cluster_id'] == cluster]\n",
    "    subset.plot(ax=ax, color=color, label=f\"Cluster {cluster}\")\n",
    "# Add legend\n",
    "ax.legend(title=\"Cluster ID\", loc='upper left', bbox_to_anchor=(1, 1))\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.title(label=\"Assigned Cluster of Refernence Points with 5 Clusters\", fontsize=12, color=\"Black\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "joined_ref_point_geop.plot(ax = ax, column= 'landcover', legend = True,  legend_kwds={'title': 'Landcover Type' ,'bbox_to_anchor':(1.1, 0.8),} )\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.title(label=\"Assigned Cluster of Refernence Points with 5 Clusters\", fontsize=12, color=\"Black\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the reference point with joined cluster_id into pandas dataframe\n",
    "joined_ref_point= joined_ref_point_geop.drop(['geometry'], axis=1)\n",
    "joined_ref_point.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# reshape the dataframe to pivot-table (to calculate the number of different land covers per cluster id)\n",
    "pivot_table = joined_ref_point.pivot_table(index=\"landcover\", columns=\"cluster_id\", values=None, aggfunc=\"size\")\n",
    "\n",
    "# build up the sub plots\n",
    "fig, axs = plt.subplots(ncols=1, nrows=len(pivot_table.columns), figsize=(9, 25))\n",
    "\n",
    "# to visualise the charts\n",
    "for i, column in enumerate(pivot_table.columns):\n",
    "    axs[i].bar(pivot_table.index, pivot_table[column])\n",
    "    axs[i].set_xlabel(\"cluster_id: \"+str(column))\n",
    "    axs[i].set_ylabel(\"landcover count\")\n",
    "    #axs[i].set_title(column)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Discussion\n",
    "\n",
    "NB. As sample points for clustering were generated randomly, so the tables and graphs below may not be aligned with the ones in the results of reruning of the codes above.\n",
    "\n",
    "### 3.1 Training for Unsupervised Classifier in GEE\n",
    "In traditional unsupervised learning we would give the model all the data and let it come up with the best clusters it sees fit, based on the number of clusters we wish to see. Given that Google Earth Engine has data for the entire globe, it is unreasonable to feed the model all the data. Even with some really nice machines, we would be sitting all day as the many possibilities are considered by the model. Creating a cluster on a subset of data, aka training the unsupervised classifier, is a pragmatic approach to getting some unsupervised learning with overloading our process. Sampleing could have been another, potentially more accurate, term for what is happening here. \n",
    "\n",
    "### 3.2 Impact of Cluster Number\n",
    "The impact of different number of clustered was studied for two different regions in Turkey. The graph presented is for Turkey region 1:\n",
    "\n",
    "![fig1.png](photos/fig1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![fig2.png](photos/fig2.png)\n",
    "\n",
    "The graph above shows that the model was run for different number of clusters, starting with a base value of 5. We utilized V-measure score to evaluate the ‘goodness’ of the result. A number close to 1 means more perfect labeling. We observe a performance increase till 15 clusters, after which the performance of the model starts to show a decline.\n",
    "\n",
    "Clusters 5, 10 and 15 were ran for another region in Turkey (Region 2) having similar land cover.\n",
    "\n",
    "![fig3.png](photos/fig3.png)\n",
    "\n",
    "The performance of different clusters in region 1 and 2 from Turkey is similar (refer Table 1). This is expected as the two regions have similar land cover types.  \n",
    "\n",
    "![table1.png](photos/table1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Clustering Time Comparison\n",
    "It was observed that the first clustering takes longer as compared to follow up clustering (refer Table 2). It was also expected that with increasing clusters, the computation time would also increase. However, no particular pattern was observed as we increased the cluster size (refer Table 3). We conclude that the run time for clustering is not directly proportional to the cluster size and might depend on other factors (e.g., data properties).\n",
    "\n",
    "![table2.png](photos/table2.png)\n",
    "![table3.png](photos/table3.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 comparing Land Cover and the Clusters\n",
    "\n",
    "The first map is the reference points from out first Turkey study area plotted with their designated cluster. In this scenario we chose to have five clusters indicated as 0, 1, 2, 3, 4. The second map is of the same study area reference points plotted with their given land cover type. While initially we expected the five landcover types to line up with the five clusters we selected, that didn’t happen. In the landcover plot there is a coastline in the lower left-hand corner indicated with several grey dots. This clear pattern is not represented within the cluster results given just above. Given our unsupervised learning model was given all the bands, not just the ones related to landcover type, to build clusters off of. The clusters encompass a profile that contains more info than just the relationship between B2, B3 and B4 (the bands related to land cover type). \n",
    "\n",
    "![fig4.png](photos/fig4.png)\n",
    "\n",
    "![fig5.png](photos/fig5.png)\n",
    "\n",
    "We took the same trained cluster and applied it to Germany to see what would happen in a totally new area. The same pattern of disconnection between cluster and land cover emerged again. For example, there are two dots in the middle forming cluster 4, they look to be totally different land cover types. Nor is there a clusters with the same sprinkle pattern of the ForestNaturalAreas. \n",
    "\n",
    "![fig6_butactually.png](photos/fig6_butactually.png)\n",
    "\n",
    "![fig7.png](photos/fig7.png)\n",
    "\n",
    "The bar charts produced below further added to this point. When we look at any bar chart created with the clusters and the land cover we see that a cluster can have mulitple different land cover types. This indicates that land cover could be a factor in the clusters but not the actual output. The goal of this lab was to practise unsupervised learning, it would be unreasonable if the model we chose was able to produce land cover exactly, let alone unprompted. In future endeavours we could consider running this excersize with only the bands that relate to land cover and see how that compares. \n",
    "\n",
    "![fig8.png](photos/fig8.png)  \n",
    "<span style=\"background-color:white;color:black\">*Figure 8. Distribution for numbers of land cover types in each of 5 clusters for Turkey*</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notice\n",
    "- Contribution: Senyang Li was mainly responsible for the coding; Aleksandra Bratic and Shreya Bansod tested the code and discussed the results.\n",
    "- AI usage: Github Capilot was used in this assignment for suggestion about selection and usage of the packages, as well as tactics for bugs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
